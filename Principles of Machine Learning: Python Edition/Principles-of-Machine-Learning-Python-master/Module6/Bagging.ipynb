{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Random Forest Models\n",
    "\n",
    "Using **ensemble methods** can greatly improve the results achieved with weak machine learning algorithms, also called **weak learners**. Ensemble methods achieve better performance by aggregating the results of many statistically independent models. This process averages out the errors and produces a final better prediction. \n",
    "\n",
    "In this lab you will work with a widely used ensemble method known as **bootstrap aggregating** or simply **bagging**. Bagging follows a simple procedure:\n",
    "1. N learners (machine learning models) are defined. \n",
    "2. N subsamples of the training data are created by **Bernoulli sampling with replacement**.\n",
    "3. The N learners are trained on the subsamples of the training data.\n",
    "4. The ensemble is scored by averaging, or taking a majority vote, of the predictions from the N learners.\n",
    "\n",
    "**Classification and regression tree models** are most typically used with bagging methods. The most common such algorithm is know as the **random forest**. The random forest method is highly scalable and generally produces good results, even for complex problems. \n",
    "\n",
    "Classification and regression trees tend to be robust to noise or outliers in the training data. This is true for the random forest algorithm as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Iris dataset\n",
    "\n",
    "As a first example you will use random forest to classify the species of iris flowers. \n",
    "\n",
    "As a first step, execute the code in the cell below to load the required packages to run the rest of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from statsmodels.api import datasets\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.metrics as sklm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for these data, you will now load and plot them. The code in the cell below does the following:\n",
    "\n",
    "1. Loads the iris data as a Pandas data frame. \n",
    "2. Adds column names to the data frame.\n",
    "3. Displays all 4 possible scatter plot views of the data. \n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iris(iris):\n",
    "    '''Function to plot iris data by type'''\n",
    "    setosa = iris[iris['Species'] == 'setosa']\n",
    "    versicolor = iris[iris['Species'] == 'versicolor']\n",
    "    virginica = iris[iris['Species'] == 'virginica']\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "    x_ax = ['Sepal_Length', 'Sepal_Width']\n",
    "    y_ax = ['Petal_Length', 'Petal_Width']\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax[i,j].scatter(setosa[x_ax[i]], setosa[y_ax[j]], marker = 'x')\n",
    "            ax[i,j].scatter(versicolor[x_ax[i]], versicolor[y_ax[j]], marker = 'o')\n",
    "            ax[i,j].scatter(virginica[x_ax[i]], virginica[y_ax[j]], marker = '+')\n",
    "            ax[i,j].set_xlabel(x_ax[i])\n",
    "            ax[i,j].set_ylabel(y_ax[j])\n",
    "            \n",
    "## Import the dataset  \n",
    "iris = datasets.get_rdataset(\"iris\")\n",
    "iris.data.columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Species']\n",
    "\n",
    "## Plot views of the iris data            \n",
    "plot_iris(iris.data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Setosa (in blue) is well separated from the other two categories. The Versicolor (in orange) and the Virginica (in green) show considerable overlap. The question is how well our classifier will separate these categories. \n",
    "\n",
    "Scikit Learn classifiers require numerically coded numpy arrays for the features and as a label. The code in the cell below does the following processing:\n",
    "1. Creates a numpy array of the features.\n",
    "2. Numerically codes the label using a dictionary lookup, and converts it to a numpy array. \n",
    "\n",
    "Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = np.array(iris.data[['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']])\n",
    "\n",
    "levels = {'setosa':0, 'versicolor':1, 'virginica':2}\n",
    "Labels =  np.array([levels[x] for x in iris.data['Species']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code in the cell below to split the dataset into test and training set. Notice that unusually, 100 of the 150 cases are being used as the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 100)\n",
    "X_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "X_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is always the case with machine learning, numeric features  must be scaled. The code in the cell below performs the following processing:\n",
    "\n",
    "1. A Zscore scale object is defined using the `StandarScaler` function from the Scikit Learn preprocessing package. \n",
    "2. The scaler is fit to the training features. Subsequently, this scaler is used to apply the same scaling to the test data and in production. \n",
    "3. The training features are scaled using the `transform` method. \n",
    "\n",
    "Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = preprocessing.StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will define and fit a random forest model. The code in the cell below defines random forest model with 5 trees using the `RandomForestClassifer` function from the Scikit Learn ensemble  package, and then fits the model. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(444)\n",
    "rf_clf = RandomForestClassifier(n_estimators=5)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the many hyperparameters of the random forest model object are displayed. \n",
    "\n",
    "Next, the code in the cell below performs the following processing to score the test data subset:\n",
    "1. The test features are scaled using the scaler computed for the training features. \n",
    "2. The `predict` method is used to compute the scores from the scaled features. \n",
    "\n",
    "Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scale.transform(X_test)\n",
    "scores = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to evaluate the model results. Keep in mind that the problem has been made difficult deliberately, by having more test cases than training cases. \n",
    "\n",
    "The iris data has three species categories. Therefore it is necessary to use evaluation code for a three category problem. The function in the cell below extends code from pervious labs to deal with a three category problem. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_metrics_3(labels, scores):\n",
    "   \n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score Setosa   Score Versicolor    Score Virginica')\n",
    "    print('Actual Setosa      %6d' % conf[0,0] + '            %5d' % conf[0,1] + '             %5d' % conf[0,2])\n",
    "    print('Actual Versicolor  %6d' % conf[1,0] + '            %5d' % conf[1,1] + '             %5d' % conf[1,2])\n",
    "    print('Actual Vriginica   %6d' % conf[2,0] + '            %5d' % conf[2,1] + '             %5d' % conf[2,2])\n",
    "    ## Now compute and display the accuracy and metrics\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    print(' ')\n",
    "    print('          Setosa  Versicolor  Virginica')\n",
    "    print('Num case   %0.2f' % metrics[3][0] + '     %0.2f' % metrics[3][1] + '      %0.2f' % metrics[3][2])\n",
    "    print('Precision   %0.2f' % metrics[0][0] + '      %0.2f' % metrics[0][1] + '       %0.2f' % metrics[0][2])\n",
    "    print('Recall      %0.2f' % metrics[1][0] + '      %0.2f' % metrics[1][1] + '       %0.2f' % metrics[1][2])\n",
    "    print('F1          %0.2f' % metrics[2][0] + '      %0.2f' % metrics[2][1] + '       %0.2f' % metrics[2][2])\n",
    "    \n",
    "print_metrics_3(y_test, scores)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice the following:\n",
    "1. The confusion matrix has dimension 3X3. You can see that most cases are correctly classified with only a few errors. \n",
    "2. The overall accuracy is 0.94. Since the classes are roughly balanced, this metric indicates relatively good performance of the classifier, particularly since it was only trained on 50 cases. \n",
    "3. The precision, recall and  F1 for each of the classes is quite good.\n",
    "|\n",
    "To get a better feel for what the classifier is doing, the code in the cell below displays a set of plots showing correctly (as '+') and incorrectly (as 'o') cases, with the species color-coded. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_iris_score(iris, y_test, scores):\n",
    "    '''Function to plot iris data by type'''\n",
    "    ## Find correctly and incorrectly classified cases\n",
    "    true = np.equal(scores, y_test).astype(int)\n",
    "    \n",
    "    ## Create data frame from the test data\n",
    "    iris = pd.DataFrame(iris)\n",
    "    levels = {0:'setosa', 1:'versicolor', 2:'virginica'}\n",
    "    iris['Species'] = [levels[x] for x in y_test]\n",
    "    iris.columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Species']\n",
    "    \n",
    "    ## Set up for the plot\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "    markers = ['o', '+']\n",
    "    x_ax = ['Sepal_Length', 'Sepal_Width']\n",
    "    y_ax = ['Petal_Length', 'Petal_Width']\n",
    "    \n",
    "    for t in range(2): # loop over correct and incorect classifications\n",
    "        setosa = iris[(iris['Species'] == 'setosa') & (true == t)]\n",
    "        versicolor = iris[(iris['Species'] == 'versicolor') & (true == t)]\n",
    "        virginica = iris[(iris['Species'] == 'virginica') & (true == t)]\n",
    "        # loop over all the dimensions\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                ax[i,j].scatter(setosa[x_ax[i]], setosa[y_ax[j]], marker = markers[t], color = 'blue')\n",
    "                ax[i,j].scatter(versicolor[x_ax[i]], versicolor[y_ax[j]], marker = markers[t], color = 'orange')\n",
    "                ax[i,j].scatter(virginica[x_ax[i]], virginica[y_ax[j]], marker = markers[t], color = 'green')\n",
    "                ax[i,j].set_xlabel(x_ax[i])\n",
    "                ax[i,j].set_ylabel(y_ax[j])\n",
    "\n",
    "plot_iris_score(X_test, y_test, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these plots. You can see how the classifier has divided the feature space between the classes. Notice that most of the errors occur in the overlap region between Virginica and Versicolor. This behavior is to be expected.  \n",
    "\n",
    "Is it possible that a random forest model with more trees would separate these cases better? The code in the cell below uses a model with 40 trees (estimators). This model is fit with the training data and displays the evaluation of the model. \n",
    "\n",
    "Execute this code and answer **Question 1** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(444)\n",
    "rf_clf = RandomForestClassifier(n_estimators=40)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "scores = rf_clf.predict(X_test)\n",
    "print_metrics_3(y_test, scores) \n",
    "plot_iris_score(X_test, y_test, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are slightly better than for the model with 5 trees. However, this small difference in performance is unlikely to be significant. \n",
    "\n",
    "Like most tree-based models, random forest models have a nice property that **feature importance** is computed during model training. Feature importance can be used as a feature selection method. \n",
    "\n",
    "Execute the code in the cell below to display a plot of the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = rf_clf.feature_importances_\n",
    "plt.bar(range(4), importance, tick_label = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Feature importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the plot displayed above. Notice that the Speal_Lenght and Sepal_Width have rather low importance. \n",
    "\n",
    "Should these features be dropped from the model? To find out, you will create a model with a reduced feature set and compare the results. As a first step, execute the code in the cell below to create training and test datasets using the reduced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create reduced feature set\n",
    "Features = np.array(iris.data[['Petal_Length', 'Petal_Width']])\n",
    "\n",
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 100)\n",
    "X_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "X_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code in the cell below to define the model, fit the model, score the model and print the results. \n",
    "\n",
    "Once you have executed the code, answer **Question 2** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(444)\n",
    "rf_clf = RandomForestClassifier(n_estimators=40)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "scores = rf_clf.predict(X_test)\n",
    "print_metrics_3(y_test, scores) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are nearly the same as those obtained with the full feature set. In all likelihood, there is no significant difference. Given that a simpler model is more likely to generalize, this model is preferred. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example\n",
    "\n",
    "Now, you will try a more complex example using the credit scoring data. You will use the prepared data which had the following preprocessing:\n",
    "1. Cleaning missing values.\n",
    "2. Aggregating categories of certain categorical variables. \n",
    "3. Encoding categorical variables as binary dummy variables.\n",
    "4. Standardizing numeric variables. \n",
    "\n",
    "Execute the code in the cell below to load the features and labels as numpy arrays for the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = np.array(pd.read_csv('Credit_Features.csv'))\n",
    "Labels = np.array(pd.read_csv('Credit_Labels.csv'))\n",
    "Labels = Labels.reshape(Labels.shape[0],)\n",
    "print(Features.shape)\n",
    "print(Labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross validation is used to estimate the optimal hyperparameters and perform model selection for the random forest model. Since random forest models are efficient to train, 10 fold cross validation is used. Execute the code in the cell below to define inside and outside fold objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(123)\n",
    "inside = ms.KFold(n_splits=10, shuffle = True)\n",
    "nr.seed(321)\n",
    "outside = ms.KFold(n_splits=10, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below estimates the best hyperparameters using 10 fold cross validation. There are a few points to note here:\n",
    "1. In this case, a grid of two hyperparameters is searched: \n",
    "  - max_features determines the maximum number of features used to determine the splits. Minimizing the number of features can prevent model over-fitted by induces bias. \n",
    "  - min_samples_leaf determines the minimum number of samples or leaves which must be on each terminal node of the tree. Maintaining the minimum number of samples per terminal node is a regularization method. Having too few samples on terminal leaves allows the model training to memorize the data, leading to high variance. Forcing too many samples on the terminal nodes leads to biased predictions. \n",
    "2. Since there is a class imbalance and a difference in the cost to the bank of misclassification of a bad credit risk customer, the \"balanced\" argument is used. The balanced argument ensures that the subsamples used to train each tree have balanced cases. \n",
    "3. The model is fit on each set of hyperparameters from the grid. \n",
    "4. The best estimated hyperparameters are printed. \n",
    "\n",
    "Notice that the model uses regularization rather than feature selection. The hyperparameter search is intended to optimize the level of regularization. \n",
    "\n",
    "Execute this code, examine the result, and answer **Question 3** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the dictionary for the grid search and the model object to search on\n",
    "param_grid = {\"max_features\": [2, 3, 5, 10, 15], \"min_samples_leaf\":[3, 5, 10, 20]}\n",
    "## Define the random forest model\n",
    "nr.seed(3456)\n",
    "rf_clf = RandomForestClassifier(class_weight = \"balanced\") # class_weight = {0:0.33, 1:0.67}) \n",
    "\n",
    "## Perform the grid search over the parameters\n",
    "nr.seed(4455)\n",
    "rf_clf = ms.GridSearchCV(estimator = rf_clf, param_grid = param_grid, \n",
    "                      cv = inside, # Use the inside folds\n",
    "                      scoring = 'roc_auc',\n",
    "                      return_train_score = True)\n",
    "rf_clf.fit(Features, Labels)\n",
    "print(rf_clf.best_estimator_.max_features)\n",
    "print(rf_clf.best_estimator_.min_samples_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will run the code in the cell below to perform the outer cross validation of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(498)\n",
    "cv_estimate = ms.cross_val_score(rf_clf, Features, Labels, \n",
    "                                 cv = outside) # Use the outside folds\n",
    "\n",
    "print('Mean performance metric = %4.3f' % np.mean(cv_estimate))\n",
    "print('SDT of the metric       = %4.3f' % np.std(cv_estimate))\n",
    "print('Outcomes by cv fold')\n",
    "for i, x in enumerate(cv_estimate):\n",
    "    print('Fold %2d    %4.3f' % (i+1, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results. Notice that the standard deviation of the mean of the AUC is more than an order of magnitude smaller than the mean. This indicates that this model is likely to generalize well. \n",
    "\n",
    "Now, you will build and test a model using the estimated optimal hyperparameters. As a first step, execute the code in the cell below to create training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(1115)\n",
    "indx = range(Features.shape[0])\n",
    "indx = ms.train_test_split(indx, test_size = 300)\n",
    "X_train = Features[indx[0],:]\n",
    "y_train = np.ravel(Labels[indx[0]])\n",
    "X_test = Features[indx[1],:]\n",
    "y_test = np.ravel(Labels[indx[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below defines a random forest model object using the estimated optimal model hyperparameters and then fits the model to the training data. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(1115)\n",
    "rf_mod = RandomForestClassifier(class_weight = \"balanced\", \n",
    "                                max_features = rf_clf.best_estimator_.max_features, \n",
    "                                min_samples_leaf = rf_clf.best_estimator_.min_samples_leaf) \n",
    "rf_mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the hyperparemeters of the random forest model object reflect those specified. \n",
    "\n",
    "The code in the cell below scores and prints evaluation metrics for the model, using the test data subset. \n",
    "\n",
    "Execute this code, examine the results, and answer **Question 4** on the course page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(probs, threshold):\n",
    "    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n",
    "\n",
    "def print_metrics(labels, probs, threshold):\n",
    "    scores = score_model(probs, threshold)\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score positive    Score negative')\n",
    "    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n",
    "    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n",
    "    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n",
    "    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n",
    "    print(' ')\n",
    "    print('           Positive      Negative')\n",
    "    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n",
    "    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n",
    "    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n",
    "    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n",
    "    \n",
    "probabilities = rf_mod.predict_proba(X_test)\n",
    "print_metrics(y_test, probabilities, 0.5)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, these performance metrics look quite good. A large majority of negative (bad credit) cases are identified at the expense of significant false positives. The reported AUC is within a standard deviation of the figure obtained with cross validation indicating that the model is generalizing well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab you have accomplished the following:\n",
    "1. Used a random forest model to classify the cases of the iris data. A model with more trees had marginally lower error rates, but likely not significantly different.\n",
    "2. Applied feature importance was used for feature selection with the iris data. The model created and evaluated with the reduced feature set had essentially the same performance as the model with more features.  \n",
    "2. Used 10 fold to find estimated optimal hyperparameters for a random forest model to classify credit risk cases. The model appears to generalize well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
