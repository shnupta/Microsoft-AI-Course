{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 5, Lab 2 - Regression\n",
    "============================\n",
    "\n",
    "In this lab, we continue from where we were in the previous lab. In case\n",
    "it's been a little while, I will re-load the data and get you up and\n",
    "running.\n",
    "\n",
    "Regression is used when you seek to *understand* or *predict* a given\n",
    "outcome variable in greater depth. One problem with correlation, as we\n",
    "saw in the previous lab, is that many of our variables overlap. In\n",
    "regression, we can isolate the unique or non-overlapping relationships\n",
    "among our variables. This is very powerful, as it allows us to\n",
    "understand what our variables are doing in a new way.\n",
    "\n",
    "In this example, we have a dataset, inspired by a dataset published on\n",
    "kaggle (<https://www.kaggle.com/unsdsn/world-happiness>). In this\n",
    "dataset, several regions of the world are compared on dimensions such as\n",
    "their generosity, happiness, GDP, and so forth.\n",
    "\n",
    "In this lab, we will use a number of Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD PACKAGES \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "=========\n",
    "\n",
    "Next, let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD DATA ####\n",
    "dat = pd.read_csv(\"datasets/regionalhappy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the dataset, we see the column names are a little messy. Further, using a `.` in a column name will lead to problems in Python as the second part of the name is not an attribute of the first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rename them easily by assigning them to the  `columns` attribute of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.columns = [\"Happiness\", \"GDP\", \"Family\", \"Life.Expect\", \"Freedom\", \"Generosity\", \"Trust.Gov\", \"Dystopia\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first few rows of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better.\n",
    "\n",
    "Regression Concepts\n",
    "===================\n",
    "\n",
    "Recall that the purpose of regression is to predict one variable. In\n",
    "this case, let's predict `Happiness`. In regression, this is done by\n",
    "generating an equation that predicts the outcome from one or more\n",
    "predictors. The equation looks like this for a single predictor:\n",
    "\n",
    "$$Y' = b_0 + b_1 x$$\n",
    "\n",
    " In this regression equation, we can guess each person's Y score (*Y*′)\n",
    "by starting with a constant (i.e., intercept, *b*<sub>0</sub>) and\n",
    "adding to that their score on a predictor variable (*x*<sub>1</sub>),\n",
    "multiplied by its respective slope (*b*<sub>1</sub>).\n",
    "\n",
    "We need not only use one predictor:\n",
    "\n",
    "$$Y' = b_0 + b_1 x + b_2 x + b_3 x + b_4 x \\ldots$$\n",
    "\n",
    "In regression, you can have as many predictors as you want, and each\n",
    "one you add (assuming it's useful), should improve the accuracy of the\n",
    "prediction. As you can imagine, this is immensely useful in many data\n",
    "science applications, as it literally provides the ability to predict\n",
    "the future.\n",
    "\n",
    "For example, imagine we wanted to predict `Happiness` from a series of\n",
    "some of our variables:\n",
    "\n",
    "$$Happiness = b_0 + b_1 \\cdot GDP + b_2 \\cdot Family + b_3 \\cdot Life\\_Expect + b_4 \\cdot Freedom + \n",
    "b_5 \\cdot Generosity$$\n",
    "\n",
    "In this regression equation, we can guess each person's `Happiness`\n",
    "score by starting with the constant (i.e., intercept, *b*<sub>0</sub>)\n",
    "and adding to that their score on each of the other variables (GDP,\n",
    "Family, Life.Expect, Freedom, Generosity), multiplied by its respective\n",
    "slope\n",
    "(*b*<sub>1</sub>, *b*<sub>2</sub>, *b*<sub>3</sub>, *b*<sub>4</sub>, *b*<sub>5</sub>).\n",
    "\n",
    "In addition to helping to predict an outcome, regression is also useful\n",
    "for *understanding* the variables. By definition, the slope is the\n",
    "expected increase in the outcome (e.g., expected gain in happiness) for\n",
    "every one unit of that predictor. For example, if the slope for `GDP`\n",
    "was 2.18, then by definition I expect to gain an additional 2.18 units\n",
    "of happiness for every one unit of GDP. The larger the slope, the more\n",
    "\"potent\" the variable. A large slope means you get a *lot* of the\n",
    "outcome (e.g., a lot of happiness) out for every one unit of the\n",
    "predictor. Conversely, if the slope were zero, the predictor literally\n",
    "would not matter; adding more of that predictor would have no impact on\n",
    "the (prediction of the) outcome. So, the slope can be though of as the\n",
    "importance of that variable for your outcome.\n",
    "\n",
    "In a regression analysis, all that is needed is the set of *b*s.\n",
    "\n",
    "Let's try this with a basic regression with one predictor. Let's predict\n",
    "`Happiness` from `GDP`.\n",
    "\n",
    "A Simple Regression\n",
    "===================\n",
    "\n",
    "There are many Python packages for computing linear regression models in Python. In this case we will use the `OLS` (ordinary least squares) function from the Statsmodels package. `OLS`has two arguments, the vector for the response or dependent variable, and an array containing the one or more predictor or independent variables. Notice that in order to get an intercept term ($b_0$) the `add_constant` method must be used to add a constant column to the predictor array. A `summary` method displays a number of statistics from a `response` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a constant term to the array of predictors to get an intercept term\n",
    "predictors = sm.add_constant(dat.GDP, prepend = False)\n",
    "\n",
    "lm_mod = sm.OLS(dat.Happiness, predictors)\n",
    "res = lm_mod.fit()\n",
    "print(res.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model coefficients are displayed in this summary. The intercept is shown as `const`. Thus, our regression equation is as follows:\n",
    "\n",
    "> Happiness' = 3.2 + 2.18(GDP).\n",
    "\n",
    "That is, we can take anyone's `GDP` score and plug it into this equation\n",
    "to predict their happiness. To get a sense as to what a \"high\" happiness\n",
    "scores is, let's inspect its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Range of Happiness = { ' + str(min(dat.Happiness)) + ' ' + str(max(dat.Happiness)) + '}')\n",
    "\n",
    "ax = plt.figure(figsize=(8, 6)).gca() # define axis\n",
    "dat.Happiness.plot.hist(ax = ax, alpha = 0.6)\n",
    "plt.title('Histogram of Happiness')\n",
    "plt.xlabel('Happiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod3_Lab2_-_Regression_files/figure-markdown_strict/unnamed-chunk-8-1.png)\n",
    "Happiness scores tend to range from ~ 2-8.\n",
    "\n",
    "Let's look at the distribution of `GDP` scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(8, 6)).gca() # define axis\n",
    "dat.GDP.plot.hist(ax = ax, alpha = 0.6)\n",
    "plt.title('Histogram of GDP')\n",
    "plt.xlabel('GDP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod3_Lab2_-_Regression_files/figure-markdown_strict/unnamed-chunk-9-1.png)\n",
    "\n",
    "GDP scores tend to range from 0-2 on this scale.\n",
    "\n",
    "What if I had a relatively low `GDP` score of say, .5? My predicted\n",
    "Happiness score would be ...\n",
    "\n",
    "> Happiness' = 3.2 + 2.18 \\* 0.5\n",
    "\n",
    "Try it yourself. What do you get as the predicted loneliness score?\n",
    "\n",
    "> Loneliness' = 4.29.\n",
    "\n",
    "Side note: there is an easy way to get R to do this for you. You simply\n",
    "use `predict()`, and input the name of your model and a `data.frame()`\n",
    "giving scores on the predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new data frame with the predictor value and the constant\n",
    "new_predict = pd.DataFrame({'GDP':[0.5], 'const':[1.0]})\n",
    "## Make prediction with new values\n",
    "res.predict(new_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is handy. We can also give it several GDP scores (0.5, 0.9, 1.7). In this case we can automate adding the constant column with the `add_connstant` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new data frame with the predictor value and the constant\n",
    "new_predict = pd.DataFrame({'GDP':[0.5, 0.9, 1.0]})\n",
    "new_predict = sm.add_constant(new_predict, prepend = False)\n",
    "## Make prediction with new values\n",
    "res.predict(new_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool is often used in the real world when you have trained a model\n",
    "using old data and want to then use it to make guesses for new data.\n",
    "It's easy to save or download a data frame with information on many new\n",
    "people and place it into `predict()`. Once the model is trained, you now\n",
    "have a valuable tool for making predictors for any new person for whom\n",
    "you have data.\n",
    "\n",
    "We can also easily visualize this relationship. Because it's simply an\n",
    "intercept and slope, the predictors form a straight line. The Seaborn `regplot` function computes a regression line with confidence intervals and plots the line with a scatter plot of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(8, 8)).gca() # define axis\n",
    "sns.regplot(x=\"GDP\", y=\"Happiness\", data=dat, ax = ax)\n",
    "plt.title('Happiness vs. GDP with linear regression line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod3_Lab2_-_Regression_files/figure-markdown_strict/unnamed-chunk-12-1.png)\n",
    "\n",
    "This represents the predictions we generated earlier (e.g., for a GDP of\n",
    "0.5, the line is at height of 0.5). The grey band represents a 95% CI\n",
    "around the line. This represents the uncertainty in the steepness of our\n",
    "slope. We are 95% confident that the \"true\" slope in the population is\n",
    "somewhere in that grey band (you can imagine the blue line swiveling a\n",
    "bit within that grey band). We can easily see the relationship is\n",
    "\"significant\" (i.e., not zero, not flat) since the most shallow line we\n",
    "could fit into that grey band is still going upward. In other words, we\n",
    "are confident that the slope *is* going upwards (i.e., is not zero). If\n",
    "we had a null hypothesis that the population slope (often symbolized\n",
    "*β*) was zero (e.g., *H*<sub>0</sub> : *β* = 0), we could reject that.\n",
    "This is more informative than simply giving us a *p*-value for the\n",
    "slope; in this case, we know the slope is not zero *and* that it is\n",
    "still pretty steep, even at is most shallow plausible value. Of course,\n",
    "we can get a *p*-value for that slope and more, which we will see in\n",
    "just a moment.\n",
    "\n",
    "Inspecting the Regression\n",
    "=========================\n",
    "\n",
    "We should check our regression in more detail, see if it satisfies\n",
    "assumptions, and see if it's a good fit to the data. To get information about your regression model, we simply ask for the\n",
    "`summary()` of `mod`. To remind you of the results you already saw, the `summary` method is applied to the results of the model `res`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to be clear, there are *two* parameters in the regression model:\n",
    "the intercept (shown as `const`) and the slope coefficient (shown as `GDP`). Each exists in the population and we are\n",
    "estimating them in our sample.\n",
    "\n",
    "This information is found in the middle of the table, where it says 'coef'.\n",
    "There are two rows, one for the intercept and one for the slope. We see.\n",
    "For our data, we estimate the intercept is 3.2032. However, this is\n",
    "just an estimate of the \"real intercept\" in the population. There is a\n",
    "standard error, *t*-test, and *p*-value (in R, called `Pr(>|t|)`) shown.\n",
    "As we see, the intercept is significant, meaning that we are confident\n",
    "it is \"not zero.\" Of course, we don't really care about the intercept,\n",
    "but the information is there.\n",
    "\n",
    "More interesting is the slope for `GDP`. We see that the slope is\n",
    "estimated at 2.1842. Again, that is just an estimate of the \"real\"\n",
    "slope in the population. However, we see there is a *t*-test and\n",
    "*p*-value and again, we see it is significant. (Same information given\n",
    "to us by the 95% CI \"grey band\" in the graph above). This *is* useful\n",
    "information. This tells you that you can be confident that the slope is\n",
    "\"not zero\"...in other words, `GDP` is 'predicting' `Happiness` The slope\n",
    "can also be interpreted directly. For each unit of GDP, we expect\n",
    "Happiness to change by 2.1842 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that each of the coefficients have a confidence interval. These values represent 95% CIs of the probabilities of the coefficient from '2.5% to 97.5%' or a range of 95%. Another measure of the model coefficients is the standard errors of the coefficients. Finally, the t-test results in the form of t-statistic and the p-value show the significant test results of the model coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are predicting each region's happiness. How accurate are those\n",
    "predictions? To answer that, we need to discuss residuals.\n",
    "\n",
    "Residuals are the difference between the actual value and the predicted value. One powerful method for understanding residuals is to make a scatter plot with the residual values on the vertical axis and the predicted values or *Score* on the horizontal axis. This type of display is known as a *residual plot*. The code in the cell below computes predicted or score values, computes residuals and creates the residual plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new data frame with the predictor value and the constant\n",
    "new_predict = dat.GDP\n",
    "new_predict = sm.add_constant(new_predict, prepend = False)\n",
    "## Make prediction with new values\n",
    "new_predict['Score'] = res.predict(new_predict)\n",
    "## Compute the residuals \n",
    "new_predict['Residuals'] = dat.Happiness - new_predict.Score\n",
    "\n",
    "ax = plt.figure(figsize=(8, 8)).gca() # define axis\n",
    "new_predict.plot.scatter(x='Score',y='Residuals', ax = ax)\n",
    "plt.title('Residuals vs. predicted value')\n",
    "plt.ylabel(\"Residual values\")\n",
    "plt.xlabel('Predicted values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two points to notice about the residual plot. First, the residual value are generally in a small range and centered on zero. Second, there does not appear to be any particular change in the dispersion (distribution) of the residuals as the predicted score changes. These observations indicate that the model is a fairly good predictor of Happiness. \n",
    "\n",
    "However, we can compute the \"residual standard error\" which is basically\n",
    "the \"average residual.\" Some books call this the \"standard error of the\n",
    "estimate.\" It is essentially the same idea as the standard deviation:\n",
    "assessing the average degree to which points vary *aroudn the line* (as\n",
    "opposed to varying around the mean, in standard deviation). In fact, it\n",
    "uses the same equation as the standard deviation, but modified so scores\n",
    "vary around the predicted value (y' ... the line) rather than the mean.\n",
    "Also, we use \"n-2\" on the bottom for reasons I won't get into:\n",
    "\n",
    "$$ RSE = \\sqrt{\\frac{\\sum \\left ( y-y' \\right )^{2}}{n-2}} $$\n",
    "\n",
    "We see in our our output, \"Residual standard error: 0.6617\", meaning our\n",
    "`Happiness` predictions are \"off\" (on average) by 0.66 points. Given\n",
    "that the scale is basically 2-7 (ish), this seems pretty good. Of\n",
    "course, we are *only* using `GDP` to predict scores right now. We could\n",
    "add other predictors and attempt to make this value smaller (i.e.,\n",
    "become more accurate in our predictions).\n",
    "\n",
    "Let's continue exploring our summary output. Residuals are important.\n",
    "More information about our residuals can be seen up top: the smallest\n",
    "residual, 1Q (the 25th percentile), the median (the 50th percentile), 3Q\n",
    "(the 75th percentile), and max. This will come in much handier later.\n",
    "The median residual is close to zero...that is very important because it\n",
    "means we are not systematically over-predicting or under-predicting\n",
    "(phew!). Also importantly, one assumption of a *valid* regression model\n",
    "is that the regressions are *normally distributed*. Of course, in a\n",
    "normal distribution, the data are symmetrical, so the lower values (1Q)\n",
    "should mirror the top values (3Q). We could also simply histogram them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(8, 6)).gca() # define axis\n",
    "new_predict.Residuals.plot.hist(ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These residuals are beautifully Normally distributed. This is a good\n",
    "sign for our model. This means that we tend to under-predict (negative\n",
    "values) just as much as we over-predict (positive values). This looks\n",
    "like a healthy and valid regression model.\n",
    "\n",
    "Referring back to the table of model evaluation statistics, we see that the \"R squared\" is .66. I won't go into\n",
    "the math behind that, but it essentially represents the proportion of\n",
    "the variance in the outcome (`Happiness`) that is explained by the\n",
    "predictor(s). the *R*<sup>2</sup> is, basically, the opposite\n",
    "of the residuals. The residuals represent *unexplained* variation in the\n",
    "happiness (Y) scores. That is, one region's happiness score may 7, yet\n",
    "we predict 6 (a residual of -1). This represents some portion of the\n",
    "score we have not yet explained. On the other hand, the *R*<sup>2</sup>\n",
    "represents the proportion of variation in `Happiness` that **is\n",
    "explained.** The better our model, the higher the *R*<sup>2</sup> and\n",
    "the smaller the residuals. If we could make our residuals zero (i.e.,\n",
    "perfectly guess every score), our *R*<sup>2</sup> would be 100% (1.00). In other words, we have explained 66% of all the variance\n",
    "in `Happiness` with just `GDP`. That further reinforces our conclusion\n",
    "that GDP is a very strong predictor of happiness.  \n",
    "\n",
    "Related to the\n",
    "Multiple R squared is a bit more information:\n",
    "`F-statistic: 297 on 1 and 153 DF,  p-value: < 1.11e-37` ... this is\n",
    "simply a significance test for the *R*<sup>2</sup> value. We see here\n",
    "that our model *is* explaining significant variation in happiness. On\n",
    "level, this is redundant to the test of significance for the slope, but\n",
    "it because very helpful when there are multiple predictors as a quick\n",
    "snapshot of the significance of the model. \n",
    "\n",
    "I bring this up only because I think it's useful to understand what\n",
    "these numbers mean. When our goal is *understanding* our variables, then\n",
    "this information is very important. We are saying we can understand 66%\n",
    "of what there is to know about happiness with GDP alone.\n",
    "\n",
    "Multiple Regression\n",
    "===================\n",
    "\n",
    "We can improve our predictions by adding more predictors. There are\n",
    "three reasons to do this.\n",
    "\n",
    "1.  Our model predicts better given more information to predict with\n",
    "2.  GDP may be caused by (and therefore serve as a proxy for) other\n",
    "    variables that are more directly relevant\n",
    "3.  GDP probably doesn't make people happy directly; it is likely\n",
    "    causing other things like health care that are more directly relevant\n",
    "\n",
    "Reasons 2 and 3 are very similar, but they differ in an important way.\n",
    "Reason 2 suggests that something may be driving *both* GDP *and*\n",
    "happiness. We discussed this possibility in the labs for association /\n",
    "causal claims. In that case, GDP is acting as a proxy for whatever that\n",
    "prior cause was...leading us to possibly misunderstand the role of GDP.\n",
    "We must statistically control for any prior causes if we want a good\n",
    "view of what effect GDP has on happiness (and even then, we can't know\n",
    "whether GDP is the cause or the effect).\n",
    "\n",
    "Reason 3, on the other hand, suggests that GDP may do things that, in\n",
    "turn, help happiness. For example, GDP may lead to better health, and\n",
    "that may aid happiness. In this case, we can still still control for\n",
    "those variables. Doing so will give \"credit\" for that relationship to\n",
    "the more proximal variable (i.e., it will appear that health, not GDP\n",
    "predicts happiness). In such a case, we should try to remember that GDP\n",
    "may still be ultimately responsible.\n",
    "\n",
    "Note that there is no way to tell the difference, statistically between\n",
    "reason 2 and 3. Thus, the data analysis needs subject matter expertise\n",
    "to draw good conclusions for observational (i.e., correlation /\n",
    "regression based) data.\n",
    "\n",
    "#### Controlling Prior Causes\n",
    "\n",
    "Reason 2 suggests that GDP may be caused by other things and that *those\n",
    "things*, not GDP are causing happiness. In such a case, failure to\n",
    "statistically adjust or control for them makes our regression is\n",
    "misleading, attributing to GDP what should rightly be attributed to\n",
    "those prior causes. For example, imagine that GDP was caused, in part,\n",
    "by freedom. In that case, we can't rightly examine the role of GDP\n",
    "without first *controlling* for freedom. Failure to do so will\n",
    "misrepresent what GDP is doing, likely inflating its importance.\n",
    "\n",
    "Importantly, however, you can easily control for a third (or fourth,\n",
    "fifth, or any number) of variables. To do so, you simply add them as\n",
    "other predictors in the regression. One very helpful feature of\n",
    "regression is that every variable is examined *controlling for / holding\n",
    "constant all the others*, so regression lets you see the *unique*\n",
    "influence of each predictor. In other words, it lets you see what the\n",
    "slope for each predictor would be, had participants had the same score\n",
    "on all the other predictors. **Important: this only works if you thought\n",
    "ahead and measured every variable you need to control for**. This is one\n",
    "reason why I advocate for collecting data and not simply relying on\n",
    "existing data repositories. You may very well need to control for a\n",
    "variable that you don't have! However, if we think ahead, we can attempt\n",
    "to measure any variable we might want to control for.\n",
    "\n",
    "Let's try it, controlling for freedom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a constant term to the array of predictors to get an intercept term\n",
    "predictors = sm.add_constant(dat[['GDP','Freedom']], prepend = False)\n",
    "\n",
    "lm_mod_2 = sm.OLS(dat.Happiness, predictors)\n",
    "res_2 = lm_mod_2.fit()\n",
    "print(res_2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, our residual standard error (average mis-prediction) has decreased\n",
    "some and our *R*<sup>2</sup> has increased. That tells us we have\n",
    "\"added\" to our ability to predict happiness. \n",
    "\n",
    "As a side note, you can\n",
    "test whether that increase is significant with `anova_lm(mod, mod2)` from the statsmodels.stats.anova package:\n",
    "\n",
    "****\n",
    "**Note:** Depending on which version of the package you are using you may see some RuntimeWarnings which you can ignore.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_lm(res, res_2, typ = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs an *F* test with a significant result (*p*-value is `Pr(>F)`,\n",
    "i.e., less than .05). This tells us that adding health/life expectancy\n",
    "*significantly* (i.e., greater than chance) improved our prediction of\n",
    "happiness. \n",
    "\n",
    "The *R*<sup>2</sup> value went up by 0.08 (i.e., 8.6%, mod:\n",
    "0.66, mod2: 0.74) so we have explained an additional 8.6% of\n",
    "happiness.\n",
    "\n",
    "Our new regression equation is:\n",
    "\n",
    "> Happiness' = 2.55 + 1.87(GDP) + 2.36(Freedom).\n",
    "\n",
    "The real action is in the slopes of the model, however. First, we can\n",
    "see that the slope for GDP is still significant as is now the slope for\n",
    "freedom (*p* values in far right column of the summary output). That\n",
    "tells us we can be confident that, to some degree, they each predict\n",
    "happiness *with the other controlled*. In other words, it is not the\n",
    "case that the role of GDP is actually all due to freedom or vice versa.\n",
    "Each has some *unique* or independent relationship with happiness.\n",
    "\n",
    "However, we also do see that the slope for GDP went down. It was\n",
    "originally 2.184 and is now 1.874, a difference of 0.31. If\n",
    "we take that as a fraction of the original slope,\n",
    "$\\frac{0.31}{2.184} = .14$, we see that our slope has gone down\n",
    "by 14%. In other words, 14% of what we had previously attributed to GDP\n",
    "we can now actually attribute to freedom. This shows just how important\n",
    "it is to control for things: we were seriously overestimating the\n",
    "importance of GDP.\n",
    "\n",
    "This illustrates the importance of having the correct statistical\n",
    "controls in your study. We could easily imagine relationships that could\n",
    "be reduced to zero when the necessary variables are controlled. If we\n",
    "want to measure, as best we can, the impact of a predictor on the\n",
    "outcome it is a **requirement** that we control for any possible hidden\n",
    "drivers of your predictor that might *actually* be responsible for your\n",
    "outcome. Thus, in this case, we might think of any *other* variables\n",
    "that might lead to *both* higher GDP and greater national happiness.\n",
    "Freedom is one, but we cannot draw a sound conclusions without seriously\n",
    "thinking through and controlling for any others.\n",
    "\n",
    "This brings up a *central* reason why organizations should conduct\n",
    "research and not merely rely on existing data. In existing data sets,\n",
    "you are tempted to draw conclusions on the basis of the data you have,\n",
    "not the data you don't have. Yet, we see here, the data you don't have\n",
    "could be necessary as a statistical control. Without that, how can we\n",
    "really know that it is GDP, and not some other prior cause of GDP that\n",
    "is driving up happiness? It is very important to seriously consider what\n",
    "variables might be causing *both* your predictor and your outcome; if\n",
    "these are not statistically controlled, then false conclusions are\n",
    "possible.\n",
    "\n",
    "#### Standardized Slopes\n",
    "\n",
    "At this point, you might also be interested in which variable is the\n",
    "stronger predictor. Unfortunately, the slopes are entirely dependent on\n",
    "the units measured.\n",
    "\n",
    "However, we can standardize them, putting them on the same scale so they\n",
    "can be directly compared. This is done by taking each slope and\n",
    "multiplying it by the ratio of the standard deviations:\n",
    "\n",
    "$$b = \\frac{SD\\_x}{SD\\_y}$$\n",
    "\n",
    "Functionally, this changes nothing. **It only changes the units.** For\n",
    "example, the slope for freedom was originally 2.3557. This means that\n",
    "happiness is expected to increase 2.3557 *units* per *unit* of freedom.\n",
    "If we multiply by standard deviations, however, the units become\n",
    "standard deviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adjusted coefficient for GDP = ' + str(res_2.params[0] * np.std(dat.GDP)/np.std(dat.Happiness)))\n",
    "print('Adjusted coefficient for Freedom = ' + str(res_2.params[1] * np.std(dat.Freedom)/np.std(dat.Happiness)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediately see that GDP has a much larger estimated effect than\n",
    "freedom. At least in our sample data, the wealth of a region is more\n",
    "important for its happiness than its freedom (the difference between\n",
    "slopes can also be tested for significance, ideally with something\n",
    "called bootstrapping, but we will not explore that now)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the tools we need to begin to understand our data. Using\n",
    "multiple regression, I can examine the relationships between any\n",
    "variables, controlling for any number of others. If I want to be able to\n",
    "assess what variables are the most important predictors, standardized\n",
    "slopes offer an easy solution (they are often comparable to correlations\n",
    "as well, providing a rough mental guide for interpreting them).\n",
    "\n",
    "#### Assumptions of Regression\n",
    "\n",
    "There are a few important assumptions and caveats you should know. Seven\n",
    "key assumptions are:\n",
    "\n",
    "Our assumptions are:\n",
    "\n",
    "1.  Linear model is appropriate\n",
    "2.  Homoskedasticity\n",
    "3.  Observations are independent\n",
    "4.  Our residuals must be normally distributed\n",
    "5.  Our residuals should have a mean of zero\n",
    "6.  The model is correctly specified\n",
    "7.  Multicollinearity is not a problem\n",
    "\n",
    "**Assumption 1**, is a linear model appropriate? Well, do we have good\n",
    "reason to suspect a non-linear relationship between GDP and happiness?\n",
    "In this case, I have no theoretical or evidence based reason to think\n",
    "otherwise. However, obviously, a linear model (i.e., a \"straight line\"\n",
    "regression) would not make sense if the data were not a straight line.\n",
    "\n",
    "Note that there are a few cases where this *IS* violated.\n",
    "\n",
    "1.  If your y-variable has only two levels (e.g., \"purchased product,\n",
    "    did not purchase product\"), then a linear model is not appropriate\n",
    "    (we use something called *logistic regression*). There are similar\n",
    "    extensions for variables with several categories.\n",
    "\n",
    "2.  Another common case is if your y-variable is a \"count\" of something\n",
    "    (e.g., how many pairs of shoes do people have?). In that case,\n",
    "    negative numbers are impossible, and any slopes will need to \"bend\"\n",
    "    as they approach the x-axis so they don't cross it. There are\n",
    "    \"count\" models for that. We won't learn them, but you could if you\n",
    "    needed to.\n",
    "\n",
    "3.  Math allows for any shape of line; for similar reasons, we can make\n",
    "    a regression line into any shape (not covered here). A word of\n",
    "    caution: the more complex a model, the more likely it is to be a\n",
    "    \"false positive\" finding. Beware making models too complex on the\n",
    "    basis of your data. Usually, simpler models are more correct unless\n",
    "    you have good reason to make it non-linear.\n",
    "\n",
    "**Assumption 2**, homoskedasicity. Briefly, this means that the\n",
    "residuals are the same all the way up and down the line. This is\n",
    "violated if residuals are really small at one end (line fits great) and\n",
    "really big at the other (line fits awful). Without going into detail,\n",
    "the *p*-values for the slopes use the residuals in the equations. If the\n",
    "residuals vary systematically throughout the graph, that's a problem.\n",
    "Look at the residuals plot above, however, and we see they seem roughly\n",
    "uniform up and down the line. We're fine.\n",
    "\n",
    "**Assumption 3**, our observations are independent. That is, our data\n",
    "points didn't influence each other. This would be a problem, for\n",
    "example, if our data included the same regions over a multi-year period.\n",
    "The 2016 happiness score and the 2017 happiness score for a region will\n",
    "be similar because it's the same region. This influences the residuals,\n",
    "and in turn, the *p*-values in a problematic way. *Violating this\n",
    "assumption tends to make your findings **more** significant* so it's\n",
    "good to account for any kind of \"nesting\" substructure.\n",
    "\n",
    "Having multiple data points on the same observation is one common way\n",
    "you can violate this assumption, but other kinds of nesting might as\n",
    "well. For example, data may be nested within stores in a way we might\n",
    "want to account for. In such a case, a *linear mixed model* is often\n",
    "better and simply extends the regression somewhat to account for the\n",
    "nesting.\n",
    "\n",
    "**Assumption 4 + 5,** our residuals must be normally distributed and\n",
    "with a mean of zero. We can briefly check this with a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new data frame with the predictor values and the constant\n",
    "new_predict = dat[['GDP','Freedom']]\n",
    "new_predict = sm.add_constant(new_predict, prepend = False)\n",
    "## Make prediction with new values and 2 predictor model\n",
    "new_predict['Score'] = res_2.predict(new_predict)\n",
    "## Compute the residuals \n",
    "new_predict['Residuals'] = dat.Happiness - new_predict.Score\n",
    "\n",
    "## Plot the histogram of the residuals\n",
    "ax = plt.figure(figsize=(8, 6)).gca() # define axis\n",
    "new_predict.Residuals.plot.hist(ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod3_Lab2_-_Regression_files/figure-markdown_strict/unnamed-chunk-23-1.png)\n",
    "\n",
    "It's roughly normal. We can check the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predict['Residuals'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has some skew, but it's not terrible.\n",
    "\n",
    "**The normality of residuals tends to be violated if the outcome\n",
    "(y-variable) is skewed**. There are other kinds of regressions for\n",
    "highly skewed outcome variables; there are also transformations that can\n",
    "be done to the outcome variable to make it more normal. We explored such\n",
    "transformations in the previous lab, but such a transformation is not\n",
    "needed here.\n",
    "\n",
    "**Assumption 6** states that we have included all the causes of our\n",
    "outcome in the model. Although impossible in practice, we can reasonably\n",
    "think through what variables we need to control for when running our\n",
    "regression.\n",
    "\n",
    "Now you see something important: to do regression analysis well, you\n",
    "need to understand the subject area. You need to know what to control\n",
    "for *before* the data are collected.\n",
    "\n",
    "**Assumption 7** states that multicollinearity is not a problem.\n",
    "\n",
    "Briefly, when you control for a variable (e.g., looking at GDP and\n",
    "happiness, controlling for freedom), you can cause a problem called\n",
    "multicollinearity.\n",
    "\n",
    "Consider the following situation: you want to assess the role of annual\n",
    "salary on employee engagement, controlling for monthly salary.\n",
    "Hopefully, you think that's a crazy idea because the predictor and\n",
    "control variables are redundant. It's completely nonsensical. What would\n",
    "it even mean to examine the role of your annual salary, holding your\n",
    "monthly salary constant? There would be \"nothing left\" to annual salary\n",
    "after we statistically hold monthly salary constant.\n",
    "\n",
    "Similarly, there are a lot of highly overlapping variables in research\n",
    "contexts. Imagine a survey asking customers how 'angry' they are and how\n",
    "'upset' they are after receiving poor service. Angry people are upset.\n",
    "Upset people feel angry. They're very similar. Conceptually, if you\n",
    "tried to examine anger with 'upset' controlled, you are going to remove\n",
    "a lot of the meaning in the anger variable, and vice versa. Will there\n",
    "be enough \"left\" to make a reasonable regression?\n",
    "\n",
    "When predictors overlap (correlate) too much among each other, they\n",
    "become useless. In general, it's good to avoid having high correlations\n",
    "among predictors, *r* = .80, *r* = .90, etc.). The slopes can act oddly\n",
    "in such cases, even reversing directions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
